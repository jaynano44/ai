{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d054c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:90% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
    "div.text_cell_render.rendered_html{font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:12pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
    "table.dataframe{font-size:12px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab916ae",
   "metadata": {},
   "source": [
    "<b><font size=\"6\"  color=\"red\">ch14. 웹 데이터 수집 </font></b>\n",
    "\n",
    "# 1절. BeautifulSoup과  parser\n",
    "`pip in stall bs4` 아나콘다를 설치하면 자동 설치되는 패키지 7500개에 포함.\n",
    "\n",
    "- 공식 Document site : https://beautiful-soup-4.readthedocs.io/en/latest/ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c8787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내 Local PC의 파일을 url()http://~처럼 접근\n",
    "import requests                        # http 요청 처리하는 lib\n",
    "from requests_file import FileAdapter  # file://프로토콜을 다루기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6eaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = requests.Session() # http를 요청처리하는 세션 객체\n",
    "s.mount(\"file://\",FileAdapter())\n",
    "# file:// 경로로 들어오면  이 요청은  c:(로컬파일)을 접근\n",
    "response = s.get('file:///ai/source/01_python/data/ch14_sample.html')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38cad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if response:\n",
    "    print('잘 접근하였습니다.')\n",
    "else:\n",
    "    print('접근을 못했습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b60946",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code\n",
    "    # 200 : 정상\n",
    "    # 400 : 없는페이지\n",
    "    # 406 : get,post 오류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95131df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content # 바이너리 형식의 html 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6611fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e941db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4231e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text  # HTML 파일의 텍스트 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3de1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html 파싱\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, #response.text\n",
    "                    \"html.parser\")\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.select_one('선택자') : 해당선택자 처음 하나만 \n",
    "el = soup.select_one('h1')  # 처음 나오는 h1태그 하나만\n",
    "print('el :' , el)\n",
    "print('el.text : ' , el.text)\n",
    "print('el.string : ' , el.string)\n",
    "print('el의 속성들 : ' , el.attrs)\n",
    "print('el의 id속성 :', el.attrs['id'])  # el.attrs은 딕셔너리\n",
    "print('el의 id속성 :', el.attrs.get('id'))\n",
    "#print('el의 href속성 :', el.attrs.get['id]')\n",
    "print('el의 href속성 :', el.attrs.get('href'))\n",
    "print('el의 태그이름 :',el.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.select('선택자')     : 해당선택자 모두\n",
    "els = soup.select('h1')  # h1태그를 list\n",
    "print('els :',els)\n",
    "print('els의 text들 :',[el.text for el in els] )                #els[0].text,els[1].text\n",
    "print('els의 string들 :',[el.string for el in els] )\n",
    "print('els의 속성들 :',[el.attrs for el in els])\n",
    "print('els의 class 속성들  :', [el.attrs.get('class') for el in els])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find(태그,속성)      : soup.select_one('선택자')와 유사\n",
    "print('select_one :', soup.select_one('h1.css') )\n",
    "print('find :', soup.find('h1', {'class':'css'}) ) # soup.select_one('h1.css')\n",
    "print('find :', soup.find('h1', class_='css') )\n",
    "print()\n",
    "print('select_one :', soup.select_one('h1#text') )\n",
    "print('find :', soup.find('h1', {'id':'text'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00808824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find_all(태그,속성)  : soup.select('선택자')와 유사\n",
    "print('select :',soup.select('h1.cc'))\n",
    "print('find_all :',soup.find_all('h1',class_='css'))\n",
    "print('find_all :',soup.find_all('h1',{'h1':'css'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 경우\n",
    "print('select :', soup.select('h1.css,span'))\n",
    "print('find_all :', soup.find_all(['h1','span'],{'class':'css'}))\n",
    "print('find_all :', soup.find_all(['h1','span'],class_='css') + soup.find_all('span'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 없는 엘리먼트 찾기\n",
    "print('select(빈 list) :', soup.select('a.css'))\n",
    "print('select_one(None) :',soup.select_one('a.css'))\n",
    "print('find_all(빈 list) :', soup.find_all('a',{'class','css'}))\n",
    "print('find(None) :',soup.find('a',{'class':'css'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e35588",
   "metadata": {},
   "source": [
    "# 2절. 정적 웹데이터 수집(정적 웹크롤링)\n",
    "## 2.1 BeutifulSoup을 활용한 html웹데이터수집\n",
    "### 1)환율정보 가져오기(네이버증권 -> 시장지표\n",
    "- https://finance.naver.com/marketindex/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 허용범위는 사이트마다 ~/robots.txt에서 확인\n",
    "    # Allow : / - 사이트의 모든 경로(/)에 대한 크롤링 허용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed952e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1 \n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://finance.naver.com/marketindex/'\n",
    "response = requests.get(url)\n",
    "# response.status_code\n",
    "# response.text / response.content.decode('cp949')\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3684e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법2\n",
    "from urllib.request import urlopen\n",
    "url = 'https://finance.naver.com/marketindex/'\n",
    "response = urlopen(url)\n",
    "response.status\n",
    "#  response.read() / response.read().decode('cp949') / \n",
    "soup = BeautifulSoup(response,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e1c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# div.head_info>Span.value (find함수)\n",
    "prices = []\n",
    "headinfos  =soup.find_all('div',class_ = 'head_info' )\n",
    "for headinfo in headinfos:\n",
    "    price = headinfo.find('span',class_='value')\n",
    "    #print(price.text.replace(',',''))\n",
    "    #print(float(''.join(price.text.split(','))))\n",
    "    prices.append(float(''.join(price.text.split(','))))\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac4e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# div.head_info>Span.value (select함수)\n",
    "prices = soup.select('div.head_info>span.value')\n",
    "[float(p.text.replace(',','')) for p in prices]\n",
    "#print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb1bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_lst>span.blind\n",
    "titles = soup.select('h3.h_lst>span.blind')\n",
    "for t in titles:\n",
    "    print(t.text , end='\\t')\n",
    "# print([t.text+'\\t' for t in titles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4960ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# div.head_info>span>span.blind\n",
    "units = soup.select('div.head_info>span>span.blind')\n",
    "units = [unit.text for unit in units]\n",
    "units.insert(7,'') # 7번째에 추가\n",
    "units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5573af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# div.head_info>span.blind\n",
    "statuses = soup.select('div.head_info > span.blind')\n",
    "for idx in range(len(statuses)):\n",
    "    print(statuses[idx].text, end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10945f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(titles), len(prices), len(units), len(statuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(titles)):\n",
    "    print(\"{} : {}{} - {}\".format(titles[idx].text,\n",
    "                                 prices[idx].text,\n",
    "                                 units[idx],\n",
    "                                 statuses[idx].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, price, unit, status in zip(titles, prices, units, statuses):\n",
    "    print(\"{} : {}{} -{}\".format(title.text,\n",
    "                                price.text,\n",
    "                                unit,\n",
    "                                status.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ec377",
   "metadata": {},
   "source": [
    "### 2) 이번주 로또 번호 출력\n",
    "- https://dhlottery.co.kr/gameResult.do?method=byWin  (google "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://dhlottery.co.kr/gameResult.do?method=byWin'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법2\n",
    "from urllib.request import urlopen\n",
    "url = 'https://dhlottery.co.kr/gameResult.do?method=byWin'\n",
    "response = urlopen(url)\n",
    "response.status\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de05a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = soup.select_one('div.win_result strong').text\n",
    "date  = soup.select_one('div.win_result > p.desc').text\n",
    "# date = soup.find('p', class_='desc').text\n",
    "lotto_number_el = soup.select('div.num.win  span')\n",
    "lotto_number = [int(el.text) for el in lotto_number_el]\n",
    "bonus_number = int(soup.select_one('div.num.bonus > p > span').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(times, date)\n",
    "print('당첨번호 ',lotto_number)\n",
    "print('보 너 스 ', bonus_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62451d48",
   "metadata": {},
   "source": [
    "### 3)다음 검색 리스트\n",
    "```\n",
    "\n",
    "no  title                href\n",
    "0   한풀 꺾인 비트코인     https://v.daum.net/v/20251110094711892\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2918b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "word = '비트코인'\n",
    "url = 'https://search.daum.net/search?w=news&nil_search=btn&DA=PGD&enc=utf8&cluster=y&cluster_page=1&q='+word\n",
    "print(url)\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "items_find_list = [] # 검색한 결과를 담을 리스트(딕셔너리 list)\n",
    "items_el = soup.select('div.item-title > strong.tit-g > a')\n",
    "for idx, item in enumerate(items_el):\n",
    "#     print(idx, item.text, item.attrs.get('href'))\n",
    "    items_find_list.append({'no':idx,\n",
    "                           'title':item.text,\n",
    "                           'link':item.attrs.get('href')})\n",
    "import pandas as pd\n",
    "pd.DataFrame(items_find_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a459126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  방법2 : 불가\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote\n",
    "word =  quote('비트코인')  # url에 한글이 포한됨 경우 한글을 quote()로 변환\n",
    "url = 'https://search.daum.net/search?w=news&nil_search=btn&DA=PGD&enc=utf8&cluster=y&cluster_page=1&q='+word\n",
    "print(url)\n",
    "response = urlopen(url)\n",
    "soup = BeautifulSoup(response,\"html.parse\")\n",
    "\n",
    "items_find_list = [] # 검색한 결과를 담을 리스트(딕셔너리 list)\n",
    "items_el = soup.select('div.item-title > strong.tit-g > a')\n",
    "for idx, item in enumerate(items_el):\n",
    "#     print(idx, item.text, item.attrs.get('href'))\n",
    "    items_find_list.append({'no':idx,\n",
    "                           'title':item.text,\n",
    "                           'link':item.attrs.get('href')})\n",
    "import pandas as pd\n",
    "pd.DataFrame(items_find_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5664f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1 이어서.. 딕셔너리 대신 리스트로\n",
    "items_find_list = [] # 검색한 결과를 담을 리스트(딕셔너리 list)\n",
    "items_el = soup.select('div.item-title > strong.tit-g > a')\n",
    "for idx, item in enumerate(items_el):\n",
    "#     print(idx, item.text, item.attrs.get('href'))\n",
    "    items_find_list.append([idx,item.text,item.attrs['href']])\n",
    "pd.DataFrame(items_find_list,columns=['번호','제목','링크'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed21bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find로 하기\n",
    "items_find_list = []\n",
    "# 'div.item-title > strong.tit-g > a'\n",
    "item_titles=soup.find_all('div',class_='item-title')\n",
    "for idx , item in enumerate(item_titles):\n",
    "    a = item.find('a')\n",
    "#     print(idx,item.text,a.attrs['href'])\n",
    "#     items_find_list.append([idx,a.text,a.attrs['href']])\n",
    "    items_find_list.append({\n",
    "        'no':idx,\n",
    "        'title':a.text,\n",
    "        'link':a.attrs['href'],\n",
    "    })\n",
    "pd.DataFrame(items_find_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d261d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 뉴스 검색 ( 원하는 키워드를 원하는 페이지를 가져오기 )\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def collect_news_list(keyword,page):\n",
    "    'keyword로 page에 다음 뉴스검색한 결과를 출력  -> list를 return'\n",
    "#     https://search.daum.net/search?w=news&nil_search=btn&DA=PGD&enc=utf8&cluster=y&cluster_page=1&q={keyword}&p={page}\n",
    "#     response = requests.get(url)\n",
    "    url = f'https://search.daum.net/search?w=news&nil_search=btn&DA=PGD&enc=utf8'\n",
    "    params = {'q':keyword,'p':page}\n",
    "    response = requests.get(url,params=params)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    items_find_list = []\n",
    "    item_titles = soup.select('div.item-title > strong.tit-g >a')\n",
    "    for idx, item in enumerate(item_titles):\n",
    "        items_find_list.append([(page-1)*10+idx,item.text,item.attrs['href']])\n",
    "    return items_find_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_news_list('동대문',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba953b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "keywords = ['청바지','동대문']\n",
    "result0 = [] # 청바지 1~3페이지까지 검색한 결과를 담을 list\n",
    "result1 = [] # 동대문 1~3페이지까지 검색한 결과를 담을 list\n",
    "pages = 3\n",
    "for i, keyword in enumerate(keywords):\n",
    "    print(f'==={i+1}번째 검색어 {keyword} 검색 결과 수집 중입니다 ===')\n",
    "    for page in range(1,pages+1):\n",
    "#         print(page,keyword)\n",
    "        if i==0:\n",
    "            result0.extend(collect_news_list(keyword,page))  # append는 결과가 3차원이 됨.=> 데이터프레임이 안됨   따라서, extend로 해야함\n",
    "        elif i==1:\n",
    "            result1.extend(collect_news_list(keyword,page))\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "result0_df = pd.DataFrame(result0,columns=['no','title','link'])\n",
    "result1_df = pd.DataFrame(result1,columns=['no','title','link'])\n",
    "result0_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "result0_df.to_csv(f'data/ch14_{keywords[0]}.csv',index=False,encoding='cp949')\n",
    "result1_df.to_csv(f'data/ch14_{keywords[1]}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d437b9",
   "metadata": {},
   "source": [
    "### 4) User-Agent를 추가하여 크롤링\n",
    "- 자동으로 브라우저를 통해 요청하는 것처럼 보이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d79b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법2 : \n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.parse import quote\n",
    "word = quote('비트코인')  #  한글을 encoding 처리\n",
    "url = 'https://search.daum.net/search?w=news&nil_search=btn&DA=PGD&enc=utf8&q='+word\n",
    "print(url)\n",
    "#headers ={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36'}\n",
    "# request =Request(url,headers=headers)\n",
    "request =Request(url)\n",
    "request.add_header('User-Agent',\n",
    "                  'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36')\n",
    "response = urlopen(request)\n",
    "soup = BeautifulSoup(response,'html.parser')\n",
    "#soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bbb73c",
   "metadata": {},
   "source": [
    "### * User-Agent로만 가능한 사이트 :방법2\n",
    "```\n",
    "- https://www.melon.com/chart/index.htm\n",
    "    :  https://www.melon.com/robots.txt에서 User-Agent에서 봇이 지정\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1 \n",
    "import  requests\n",
    "url = 'https://www.melon.com/chart/index.htm'\n",
    "melonpage = requests.get(url)\n",
    "melonpage\n",
    "# soup =BeautifulSoup(melonpage.text,'html.parser')\n",
    "# soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea1118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법 2 : User-Agent 추가\n",
    "from urllib.request import urlopen, Request\n",
    "url = 'https://www.melon.com/chart/index.htm'\n",
    "# melonpage = urlopen(url) # 에러남\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36'}\n",
    "request = Request(url,headers=headers)\n",
    "melonpage = urlopen(request)\n",
    "soup = BeautifulSoup(melonpage,'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da2d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1 : User-Agent 추가\n",
    "import  requests\n",
    "url = 'https://www.melon.com/chart/index.htm'\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36'}\n",
    "melonpage = requests.get(url,headers=headers)\n",
    "soup = BeautifulSoup(melonpage.text, #melonpage.\n",
    "                    'html.parser')\n",
    "# soup\n",
    "# div.wrap.t_center > span.rank\n",
    "ranks = soup.select('td div.wrap.t_center > span.rank')\n",
    "titles =soup.select('div.ellipsis.rank01 > span') # title.text.strip()\n",
    "# [title.text.strip() for title in titles]\n",
    "singers =soup.select('div.ellipsis.rank02') #[singer.text.strip()[:20]\n",
    "# [singer.text.strip()[:20] for singer in singers]\n",
    "# 1위 | 노래 제목 - 가수명\n",
    "for rank,title,singer in zip(ranks,titles,singers):\n",
    "    print(f'{rank.text}위 | {title.text.strip()} - {singer.text.strip()[:20]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4720b5e5",
   "metadata": {},
   "source": [
    "### 5)네이버 지식인으로 검색(open API 사용 X)\n",
    "-특정 keyword를 특정페이지수만큼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "keyword = '쳇지피티'\n",
    "# url =f' https://kin.naver.com/search/list.nave'\n",
    "# params = {'query':keyword}\n",
    "# response = get(url,params=params)\n",
    "\n",
    "url =f' https://kin.naver.com/search/list.naver?query={keyword}'\n",
    "print(url)\n",
    "response = get(url)\n",
    "print(response.status_code)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "#soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb58873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법2 \n",
    "from urllib.request import  urlopen\n",
    "from urllib.parse import quote  # keyword에 한글이 있을 경우\n",
    "keyword = quote('쳇지피티')\n",
    "url = f' https://kin.naver.com/search/list.naver?query={keyword}'\n",
    "print(url)\n",
    "response =urlopen(url)\n",
    "print(response.status)\n",
    "soup = BeautifulSoup(response,'html.parser')\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이징 포함(1~3 페이지까지 가져오기)\n",
    "from urllib.request import  urlopen\n",
    "from urllib.parse import quote  # keyword에 한글이 있을 경우\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "keyword = quote('쳇지피티')\n",
    "page= 3\n",
    "items_list = [] # 크롤링한 데이터를 담을 list(title,link)\n",
    "for page in range(1,page+1):\n",
    "    # print(page)\n",
    "    url = f'https://kin.naver.com/search/list.naver?query={keyword}&page={page}'\n",
    "    #url = f'https://kin.naver.com/search/list.naver?query=%EC%B3%87%EC%A7%80%ED%94%BC%ED%8B%B0&page=3'\n",
    "    #print(url)\n",
    "    response = urlopen(url)\n",
    "    #print(response.status) \n",
    "    soup = BeautifulSoup(response,'html.parser')\n",
    "    items = soup.select('dt > a')\n",
    "    #print(len(items))\n",
    "     #print(items)\n",
    "    for item in items:\n",
    "        title = item.text\n",
    "        link = item.attrs.get('href')\n",
    "        #print(title,link)\n",
    "        items_list.append({\n",
    "            'title':title,\n",
    "            'link':link\n",
    "        })\n",
    "#items_list  가독성떨어짐\n",
    "df = pd.DataFrame(items_list)\n",
    "print(df.shape)\n",
    "print(df.loc[29,'link'])\n",
    "df.sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6d4e2",
   "metadata": {},
   "source": [
    "## 2.2 openAPI사용 : json웹데이터 수집\n",
    "### 1) 네이버 지식인으로 검색 (open API 사용 O)\n",
    "-네이버 개발자샌터에서 애플리캐이션등록(이름/검색/WEB설정 http://localhost)\n",
    "- .env파일에 CLIENT_ID/CLIENT_SECRET 환경변수 저장\n",
    "- 환경변수를 읽기 위해서  `pip install dotenv`\n",
    "- 특정 keyword를 지식검색(데이터수 30개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2268ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  환경변수 읽어오기\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()   # 현 소스와 같은 폴더 내의 .env를 메모리에 load\n",
    "                # 다른 경우 load_dotenv(dorenv_path='.env')\n",
    "# print(os.getenv('CLIENT_ID'))\n",
    "# print(os.getenv('CLIENT_SECRET'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법2\n",
    "import os\n",
    "import urllib.request\n",
    "import json\n",
    "client_id = os.getenv('CLIENT_ID')\n",
    "client_secret = os.getenv('CLIENT_SECRET')\n",
    "encText = urllib.parse.quote(\"쳇지피티\")\n",
    "url = \"https://openapi.naver.com/v1/search/kin?query=\" + encText # JSON 결과\n",
    "# url = \"https://openapi.naver.com/v1/search/kin.xml?query=\" + encText # XML 결과\n",
    "# request = urllib.request.Request(url)\n",
    "# request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "# request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "\n",
    "headers = {\n",
    "    'X-Naver-Client-Id':client_id,\n",
    "    'X-Naver-Client-Secret':client_secret\n",
    "}\n",
    "request = urllib.request.Request(url,headers=headers)\n",
    "\n",
    "response = urllib.request.urlopen(request)\n",
    "\n",
    "rescode = response.status\n",
    "if(rescode==200):\n",
    "    response_body = response.read()       #json파일\n",
    "    #print(type(response_body.decode('utf-8')))\n",
    "    #response_body의 문자내용을 json스타일의 딕셔너리로 변환    print(json.loads(response_body.decode('utf-8')))\n",
    "    print(response_body.decode('utf-8')[:50])  # response_body.decode('utf-8') :json스타일의 문자\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)\n",
    "for item in items:\n",
    "    title = item['title'].replace('<b>','').replace('</b>','')\n",
    "    link  = item.get('link')\n",
    "    description = item.get('description').replace('<b>','').replace('</b>','')\n",
    "    items_list.append([title, link, description])\n",
    "pd.DataFrame(items_list, columns=['title', 'link', 'description']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45872cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json # response 택스트를 json스타일의 딕셔너리로 \n",
    "client_id = os.getenv('CLIENT_ID')\n",
    "client_secret = os.getenv('CLIENT_SECRET')\n",
    "query = '쳇지피티'\n",
    "headers = {\n",
    "    'X-Naver-Client-Id':client_id,\n",
    "    'X-Naver-Client-Secret':client_secret\n",
    "}\n",
    "# url = f\"https://openapi.naver.com/v1/search/kin?query={query}&display=30\"\n",
    "# response = requests.get(url,headers=headers)\n",
    "url = \"https://openapi.naver.com/v1/search/kin\"\n",
    "params = {'query':query,'display':30}\n",
    "response = requests.get(url,params=params,headers=headers)\n",
    "# print(response.text[:500])\n",
    "# items = json.loads(response.text)['items']\n",
    "items = response.json()['items']\n",
    "items_list = []\n",
    "for item in items:\n",
    "    title =item['title'].replace('<b>','').replace('</b>','')\n",
    "    link = item.get('link')\n",
    "    description = item.get('description').replace('<b>','').replace('</b>','')\n",
    "    items_list.append([title,link,description])\n",
    "pd.DataFrame(items_list,columns=['title','link','description']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa29fa",
   "metadata": {},
   "source": [
    "### quiz)네이버 openAPI를 이용해서 청바지 이미지 100건의 데이터를 ch14_청바지.csv출력\n",
    "```\n",
    "    제목, 링크, 썸네일,sizeheight,sizewidth\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39053db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "# import json # requests.json()을 사용하므로, 명시적으로 json 모듈을 사용할 필요는 줄어듭니다.\n",
    "# import urllib.parse # requests를 사용하므로 이 모듈도 필요 없습니다.\n",
    "\n",
    "client_id = os.getenv('CLIENT_ID')\n",
    "client_secret = os.getenv('CLIENT_SECRET')\n",
    "\n",
    "query = \"청바지\" # 검색어\n",
    "\n",
    "headers = {\n",
    "    'X-Naver-Client-Id': client_id,\n",
    "    'X-Naver-Client-Secret': client_secret\n",
    "}\n",
    "\n",
    "# requests.get()의 params 인자에 쿼리와 display를 넘기면 requests가 자동으로 처리해줍니다.\n",
    "# query 인코딩을 직접 하지 않아도 되어서 더 깔끔해요.\n",
    "params = {\n",
    "    'query': query,\n",
    "    'display': 100 # 한 번에 가져올 이미지 수\n",
    "}\n",
    "\n",
    "url = \"https://openapi.naver.com/v1/search/image\"\n",
    "\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "print(response) \n",
    "\n",
    "# 응답 상태 코드가 200 (성공)인지 확인\n",
    "if response.status_code == 200:\n",
    "    result_json = response.json()\n",
    "    items = result_json.get('items', [])\n",
    "    \n",
    "    # print(response.text[:500]) \n",
    "\n",
    "    items_list = []\n",
    "    for item in items:\n",
    "        title = item.get('title', '').replace('<b>','').replace('</b>','')\n",
    "        link = item.get('link', '')         # 원본 이미지 URL\n",
    "        thumbnail = item.get('thumbnail', '') # 썸네일 이미지 URL\n",
    "        sizeheight = item.get('sizeheight', '') # 이미지 높이\n",
    "        sizewidth = item.get('sizewidth', '')   # 이미지 너비\n",
    "        \n",
    "        items_list.append([title, link, thumbnail, sizeheight, sizewidth])\n",
    "    \n",
    "    # 추출한 데이터를 Pandas DataFrame으로 생성합니다.\n",
    "    df = pd.DataFrame(items_list, columns=['제목', '링크', '썸네일', '높이', '너비'])\n",
    "    print(df.head())\n",
    "    \n",
    "else:\n",
    "    # 요청 실패 시 오류 코드와 응답 내용을 출력합니다.\n",
    "    print(f\"Error Code: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb4ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선생님것. 방법2\n",
    "def get_image_list(query):\n",
    "    'query로 검색한 이미지 정보(정보,링크,썸네일,크기) 100건 데이터 프레임을 return(방법2)'\n",
    "    from urllib.request import urlopen, Request\n",
    "    from urllib.parse import quote\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    load_dotenv()\n",
    "    client_id = os.getenv('CLIENT_ID')\n",
    "    client_secret = os.getenv('CLIENT_SECRET')\n",
    "    encText = quote(query)\n",
    "    url = f'https://openapi.naver.com/v1/search/image?query={encText}&display=100'\n",
    "    headers = {\n",
    "        'X-Naver-Client-Id':client_id,\n",
    "        'X-Naver-Client-Secret':client_secret\n",
    "    }\n",
    "    request = Request(url, headers=headers)\n",
    "    response = urlopen(request)\n",
    "    # print(response.read().decode('utf-8'))\n",
    "    items = json.loads(response.read().decode('utf-8'))['items']\n",
    "    items_list = []\n",
    "    for item in items:\n",
    "        #print(item)\n",
    "        link = item.get('link')\n",
    "        thumbnail = item.get('thumbnail')\n",
    "        items_list.append({\n",
    "            '제목':item.get('title'),\n",
    "            '링크':link,\n",
    "            '썸네일': thumbnail,\n",
    "            'sizeheight': int(item.get('sizeheight')),\n",
    "            'sizewidth':  int(item.get('sizewidth')),\n",
    "        })\n",
    "    return pd.DataFrame(items_list)\n",
    "df = get_image_list(\"청바지\")\n",
    "df.to_csv('data/ch14_청바지.csv')\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe97ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법1(선생님)\n",
    "def get_image_list(query):\n",
    "    'query로 검색한 이미지 정보(정보,링크,썸네일,크기) 100건 데이터 프레임을 return(방법1)'\n",
    "    import requests\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    load_dotenv()\n",
    "    client_id = os.getenv('CLIENT_ID')\n",
    "    client_secret = os.getenv('CLIENT_SECRET')\n",
    "    url = 'https://openapi.naver.com/v1/search/image'\n",
    "    params = {'query':query, 'display':100 }\n",
    "    headers = {\n",
    "        'X-Naver-Client-Id':client_id,\n",
    "        'X-Naver-Client-Secret':client_secret\n",
    "    }\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    #items = response.json()['items']\n",
    "    items = json.loads(response.text)['items']\n",
    "    items_list = []\n",
    "    for item in items:\n",
    "        #print(item)\n",
    "        link = item.get('link')\n",
    "        thumbnail = item.get('thumbnail')\n",
    "        items_list.append({\n",
    "            '제목':item.get('title'),\n",
    "            '링크':link,\n",
    "            '썸네일': thumbnail,\n",
    "            'sizeheight': int(item.get('sizeheight')),\n",
    "            'sizewidth':  int(item.get('sizewidth')),\n",
    "        })\n",
    "    return pd.DataFrame(items_list)\n",
    "df = get_image_list(\"청바지\")\n",
    "df.to_csv('data/ch14_청바지.csv')\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90384b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[0, '링크'])\n",
    "print(df.loc[0, '썸네일'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92857cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(attr, idx, link, query):\n",
    "    'link의 이미지를 image/attr_idx_query.확장자로 local에 저장'\n",
    "    import requests\n",
    "    file_extension = link.split('.')[-1] #확장자\n",
    "    quote_index = file_extension.find('?')   # 확장자 뒤에 ?가 있는 위치    .jpg?w=780\n",
    "    if quote_index != -1:\n",
    "        file_extension = file_extension[:quote_index]\n",
    "    img = requests.get(link).content  # 바이너리\n",
    "    with open(f'image/{attr}_{idx+1}_{query}.{file_extension}','wb') as f:\n",
    "        f.write(img)\n",
    "save_image('메인',0,df.loc[0, '링크'],'청바지')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합치기\n",
    "# 방법1(선생님)\n",
    "def get_image_list_save_image(query):\n",
    "    'query로 검색한 이미지 정보(정보,링크,썸네일,크기) 100건 데이터 프레임을 return(방법1)'\n",
    "    import requests\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    load_dotenv()\n",
    "    client_id = os.getenv('CLIENT_ID')\n",
    "    client_secret = os.getenv('CLIENT_SECRET')\n",
    "    url = 'https://openapi.naver.com/v1/search/image'\n",
    "    params = {'query':query, 'display':100 }\n",
    "    headers = {\n",
    "        'X-Naver-Client-Id':client_id,\n",
    "        'X-Naver-Client-Secret':client_secret\n",
    "    }\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    #items = response.json()['items']\n",
    "    items = json.loads(response.text)['items']\n",
    "    items_list = [] # 정보가 담길 list\n",
    "    for idx, item in enumerate(items):\n",
    "        #print(item)\n",
    "        link = item.get('link')\n",
    "        thumbnail = item.get('thumbnail')\n",
    "        items_list.append({\n",
    "            '제목':item.get('title'),\n",
    "            '링크':link,\n",
    "            '썸네일': thumbnail,\n",
    "            'sizeheight': int(item.get('sizeheight')),\n",
    "            'sizewidth':  int(item.get('sizewidth')),\n",
    "        })\n",
    "        #이미지 파일저장\n",
    "        save_image('메인',idx,link,query)\n",
    "        save_image('썸네일',idx,link,query)\n",
    "        if (idx%20 == 0)& (idx !=0):\n",
    "            print(f'====={idx}% 진행 중 =====')\n",
    "    print('+++++++ 완 료 ++++++++')\n",
    "    return pd.DataFrame(items_list)\n",
    "df = get_image_list_save_image(\"청바지\")\n",
    "df.to_csv('image/ch14_청바지.csv')\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3fef4c",
   "metadata": {},
   "source": [
    "### 2.3 XML 웹데이터 수집\n",
    "- RSS / open API 을 통한 XML 웹데이터 수집\n",
    "### 1) 전국 날씨 RSS를  BeautifulSoup을 이용한 크롤링\n",
    "- 구글에 \"기상청 RSS\" 검색한 첫번째 사이트 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2fcd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "items_list = []\n",
    "url = 'https://www.kma.go.kr/repositary/xml/fct/mon/img/fct_mon1rss_108_20251106.xml'\n",
    "# 방법1\n",
    "# target = requests.get(url)\n",
    "# soup = BeautifulSoup(target.text, \"xml\") # pip install lxml 필요없음\n",
    "# 방법2\n",
    "target = urlopen(url)\n",
    "soup = BeautifulSoup(target, \"xml\")\n",
    "local_tas = soup.select('local_ta')\n",
    "for local_ta in local_tas:\n",
    "    local_name = local_ta.select_one('local_ta_name').text\n",
    "    week1_normalYear = local_ta.select_one('week1_local_ta_normalYear').text\n",
    "    week1_similarRange = local_ta.select_one('week1_local_ta_similarRange').text\n",
    "    week1_minVal = local_ta.select_one('week1_local_ta_minVal').text\n",
    "    week1_similarVal = local_ta.select_one('week1_local_ta_similarVal').text\n",
    "    week1_maxVal = local_ta.select_one('week1_local_ta_maxVal').text\n",
    "    items_list.append({\n",
    "        '지역':local_name,\n",
    "        '1주평년기온':week1_normalYear,\n",
    "        '1주기온범위':week1_similarRange,\n",
    "        '1주낮을확률':week1_minVal,\n",
    "        '1주비슷할확률':week1_similarVal,\n",
    "        '1주높을확률':week1_maxVal\n",
    "    })\n",
    "pd.DataFrame(items_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95da808",
   "metadata": {},
   "source": [
    "### 2) XML응답하는  open API\n",
    "- data.go.kr에서 \n",
    "    - 서울특별시_노선정보조회 서비스(버스ID,정류장목록과정류장ID)\n",
    "    - 서울특별시_버스위치정보조회 서비스(실시간 버스 위치 목록)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b2512",
   "metadata": {},
   "source": [
    "### ☆STEP1 버스번호의 버스ID박아오기\n",
    "- 서울특별시_노선정보조회 서비스 -3번 가능(getBusRouteList) 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba42f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# print(os.getenv('KEY'))\n",
    "# print(os.getenv('KEY1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "150d0900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ws.bus.go.kr/api/rest/busRouteInfo/getBusRouteList?ServiceKey=17fe52e6614af862f58a7555493dcf98e50eb55731d19ac363d85a41ce2de47a&strSrch=162\n"
     ]
    }
   ],
   "source": [
    "# 내 pc에 저장하기\n",
    "from bs4 import BeautifulSoup\n",
    "# urlretrieve(url, 저장경로) : url의 파일을 저장경로에 저장\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.parse import quote\n",
    "# busNum = quote('구로09')\n",
    "busNum = '162'\n",
    "key = os.getenv('KEY')\n",
    "url = f'http://ws.bus.go.kr/api/rest/busRouteInfo/getBusRouteList?ServiceKey={key}&strSrch={busNum}'\n",
    "print(url)\n",
    "savefilename1 = 'data/ch14_busInfo.xml'\n",
    "urlretrieve(url,savefilename1)  # xml(url)파일을 savefilename1d으로 저장\n",
    "with open(savefilename1,encoding='utf-8') as f:\n",
    "    xml = f.read()\n",
    "soup  = BeautifulSoup(xml,'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a93834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "busRouteId =  100100034\n"
     ]
    }
   ],
   "source": [
    "#  pc에 저장하지 않고 \n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "# busNum = quote('구로09')\n",
    "busNum = '162'\n",
    "key = os.getenv('KEY')\n",
    "url1 = f'http://ws.bus.go.kr/api/rest/busRouteInfo/getBusRouteList?ServiceKey={key}&strSrch={busNum}'\n",
    "# print(url1)\n",
    "response = requests.get(url1)\n",
    "soup = BeautifulSoup(response.text,'xml')\n",
    "# soup\n",
    "\n",
    "for item in soup.select('itemList'):\n",
    "    busRouteNm = item.select_one('busRouteNm').text\n",
    "#     print(busRouteNm)\n",
    "    if busRouteNm == busNum:\n",
    "        busRouteId = item.select_one('busRouteId').text\n",
    "        break;\n",
    "print('busRouteId = ', busRouteId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea3eac",
   "metadata": {},
   "source": [
    "### ☆STEP2.  버스id로 버스정류장목록받아오기(정류장명,id,경도,위도)\n",
    "- 서울특별시_노선정보조회 서비스 -4번 기능(getStaionsByRouteList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df7ae1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ws.bus.go.kr/api/rest/busRouteInfo/getStaionByRoute?ServiceKey=17fe52e6614af862f58a7555493dcf98e50eb55731d19ac363d85a41ce2de47a&busRouteId=100100034\n",
      "162번 정류장 갯수 : 77\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>정류소명</th>\n",
       "      <th>id</th>\n",
       "      <th>경도</th>\n",
       "      <th>위도</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>정릉산장아파트</td>\n",
       "      <td>107000071</td>\n",
       "      <td>127.003343</td>\n",
       "      <td>37.616712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>정릉4동주민센터.경국사</td>\n",
       "      <td>107000073</td>\n",
       "      <td>127.006345</td>\n",
       "      <td>37.613529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>북한산보국문역2번출구</td>\n",
       "      <td>107000518</td>\n",
       "      <td>127.0079858233</td>\n",
       "      <td>37.612293899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>성북청수도서관.정릉4동성당</td>\n",
       "      <td>107000075</td>\n",
       "      <td>127.0084193769</td>\n",
       "      <td>37.6115696748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>정릉시장입구</td>\n",
       "      <td>107000077</td>\n",
       "      <td>127.0098212542</td>\n",
       "      <td>37.6084653256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             정류소명         id              경도             위도\n",
       "0         정릉산장아파트  107000071      127.003343      37.616712\n",
       "1    정릉4동주민센터.경국사  107000073      127.006345      37.613529\n",
       "2     북한산보국문역2번출구  107000518  127.0079858233   37.612293899\n",
       "3  성북청수도서관.정릉4동성당  107000075  127.0084193769  37.6115696748\n",
       "4          정릉시장입구  107000077  127.0098212542  37.6084653256"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url2 = f'http://ws.bus.go.kr/api/rest/busRouteInfo/getStaionByRoute?ServiceKey={key}&busRouteId={busRouteId}'\n",
    "print(url2)\n",
    "response = requests.get(url2)\n",
    "soup = BeautifulSoup(response.content,'xml')\n",
    "itemLists = soup.find_all('itemList')\n",
    "print(f'{busNum}번 정류장 갯수 : {len(itemLists)}')\n",
    "bus_station = []\n",
    "for itemList in itemLists:\n",
    "    stationNm = itemList.find('stationNm').text  # 정류소이름\n",
    "    station   = itemList.find('station').text    # 정류소 id\n",
    "    gpsX      = itemList.find('gpsX').text       # 경도\n",
    "    gpsY      = itemList.find('gpsY').text       # 위도\n",
    "    bus_station.append([stationNm,station,gpsX,gpsY])\n",
    "df_station = pd.DataFrame(bus_station,columns=['정류소명','id','경도','위도'])\n",
    "df_station.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2056d9ee",
   "metadata": {},
   "source": [
    "### ☆STEP3. 버스id로 실시간위치정보를 받아오기(차량번호,혼잡도,최종정류장id,다음정류장id,도착소요시간)\n",
    "- 서울특별시_버스위치정보조회 서비스 - 2번(getBusPosByRtidList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db02c10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "운행중인 버스는 23대입니다\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url3 = f'http://ws.bus.go.kr/api/rest/buspos/getBusPosByRtid?serviceKey={key}&busRouteId={busRouteId}'\n",
    "# print(url3)\n",
    "response = requests.get(url3)\n",
    "soup = BeautifulSoup(response.text,'xml')\n",
    "itemLists = soup.select('itemList')\n",
    "print(f'운행중인 버스는 {len(itemList)}대입니다')\n",
    "bus_position = []\n",
    "for itemList in itemLists:\n",
    "    plainNo = itemList.select_one('plainNo').text  # 차량번호\n",
    "    congetion = itemList.select_one('congetion').text # 혼잡도(congetion)\n",
    "     # 0 : 없음, 3 : 여유, 4 : 보통, 5 : 혼잡, 6 : 매우혼잡\n",
    "    congetion =\"없음\" if congetion=='0'\\\n",
    "        else  '여유' if congetion =='3' \\\n",
    "        else  \"보통\" if congetion =='4' \\\n",
    "        else  '혼잡' if congetion =='5'  else '매우혼잡'\n",
    "    bus_position.append({\n",
    "        '차량번호':plainNo,\n",
    "        '혼잡도':congetion\n",
    "    })\n",
    "df_position = pd.DataFrame(bus_station)\n",
    "df_position\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525f991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc54a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614f1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5246f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ffe31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ae5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93ea8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a233ba27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef118df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be23fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d601001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
